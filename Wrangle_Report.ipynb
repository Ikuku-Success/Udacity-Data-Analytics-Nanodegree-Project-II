{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `WRANGLE_REPORT`\n",
    "The `wrangle report` shows all the data wrangling process carried out in the `wrangle_act` jupyter notebook as well as the code & comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Programmatically opening the tweet-json.txt file\n",
    "df_list = []\n",
    "with open(\"tweet-json.txt\", \"r\", encoding = 'utf-8') as file:\n",
    "    for i in file:\n",
    "        data = json.loads(i)\n",
    "        df_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "df = pd.DataFrame(df_list, columns = df_list[0].keys())\n",
    "df = df[[\"id\", \"favorite_count\"]]\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigating the archive dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for the datatypes of the archive dataset\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More investigation on the archive dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the retweeted_status_id & retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_user_id, in_reply_to_status_id\n",
    "archive = archive.drop([\"retweeted_status_timestamp\", \"retweeted_status_id\", \"retweeted_status_user_id\", \"in_reply_to_user_id\", \"in_reply_to_status_id\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to confirm the removal\n",
    "archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the image predictions dataset as predictions\n",
    "predictions = pd.read_csv(\"image_predictions.tsv\", sep = '\\t')\n",
    "predictions.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigating the predictions dataset\n",
    "predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the datatypes of the predictions dataset\n",
    "predictions.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for null values in the expanded column\n",
    "archive[archive.expanded_urls.isnull() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the rating_numerator column for the maximum value\n",
    "archive[archive.rating_numerator == archive.rating_numerator.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the rating denominator for the maximum value\n",
    "archive[archive.rating_denominator == archive.rating_denominator.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the rating numerator whose value is greater than 20\n",
    "archive[archive.rating_numerator > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using regular expressions to check the numerator values where they have been incorrectly entered\n",
    "import re\n",
    "archive[archive.text.str.contains(\"r(\\d+\\.\\d*\\/\\d+)\")][[\"text\", \"rating_numerator\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicate values in the dataset\n",
    "archive[archive.tweet_id.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value count of the ima_num column\n",
    "predictions.img_num.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplicated tweet id\n",
    "predictions[predictions.tweet_id.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplicated jpg_url\n",
    "predictions[predictions.jpg_url.duplicated()].jpg_url.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering duplicated jpg_url in the dataset\n",
    "predictions[predictions.jpg_url.apply(lambda x: x in predictions[predictions.jpg_url.duplicated()].jpg_url.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplicated  values in the twitter json converted dataset\n",
    "df[df.duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplicated id's\n",
    "df[df.id.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplicated retweet count & favourite count\n",
    "df.favorite_count.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After all the processes above was carried out, I highlighted some `Quality issues` & `Tidiness Issues` I noticed about the data as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "#### `archive table`, `predictions table`, `df table` :\n",
    "1.There are columns with missing values namely - expanded_urls\n",
    "\n",
    "2.Name, doggo, floofer, pupper and puppo columns have value with the name `None`\n",
    "\n",
    "3.Values in `source` column are not human readable\n",
    "\n",
    "4.Use of `_ `instead of `space` in p1, p2 and p3 column values. Also, values upper case sometimes lowercase other times in p1, p2 and p3 columns\n",
    "\n",
    "5.column names are not clearly descriptive\n",
    "\n",
    "6.There are image duplicate predictions present for duplicate `jpg_url` with different tweet ids and rest all the data same.\n",
    "\n",
    "7.tweet id title is different, `id` here `tweet_id` in others.\n",
    "\n",
    "8.There are `image duplicate predictions` present for duplicate `jpg_url` with different `tweet ids` and rest all the data same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness issues\n",
    "1.One variable in 4 columns - dog_stage(doggo, floofer, pupper, puppo)\n",
    "\n",
    "2.Merge the tables - `archive_tweet` and `df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, all the issues have been cleared. The data is ready to be stored and analysed to get insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data\n",
    "Save gathered, assessed, and cleaned master dataset to a CSV file named \"twitter_archive_master.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweet_data_master = pd.merge(archive_clean, predictions_clean, on = 'tweet_id', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_master.to_csv('twitter_archive_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing predictions_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_clean.to_csv('image_predictions_master.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
